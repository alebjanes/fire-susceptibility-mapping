{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Multi-input.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alebjanes/fire-susceptibility-mapping/blob/main/Multi_input.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmJS1xb-Lrd5"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import load\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Activation, Input, Concatenate, Conv2DTranspose\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow as tf\n",
        "from keras.layers.merge import concatenate\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc2i-BPJLrd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7eb8771b-023a-4436-8168-bcdaca032eb5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "7IwJmX8_LreA"
      },
      "source": [
        "#DATA\n",
        "pixel_size = 25\n",
        "mid_pixel = pixel_size/2 - 0.5\n",
        "\n",
        "Dataset = np.load('/content/drive/My Drive/MT/Samples/samples' + str(pixel_size) + 'x'+ str(pixel_size) +'_v3.npy')\n",
        "\n",
        "X = Dataset[:,:,:,1:21]\n",
        "target = Dataset[:,mid_pixel,mid_pixel,0]\n",
        "y = np.expand_dims(target, axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJjpIMW1Be9O"
      },
      "source": [
        "X=X_train\n",
        "y=y_train\n",
        "\n",
        "del y_test\n",
        "del X_test\n",
        "del target\n",
        "del X_train\n",
        "del y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b42AJ4aoLmEG"
      },
      "source": [
        "#CV\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# Define per-fold score containers <-- these are new\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "acc_per_fold_train = []\n",
        "loss_per_fold_train = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "qVsCyonMLreV"
      },
      "source": [
        "def inputlayers(input_shape, batch_normalization, momentum, activation):\n",
        "  input = Input(shape=input_shape)\n",
        "  x = Conv2D(32, kernel_size=(3, 3), activation=activation, padding = 'same')(input)\n",
        "  if batch_normalization: \n",
        "    x = BatchNormalization(momentum=0.99)(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = Conv2D(64, kernel_size=(3, 3), activation=activation, padding = 'same')(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  conv = Conv2D(128, kernel_size=(3, 3), activation=activation, padding = 'same')(x)\n",
        "\n",
        "  return input, conv\n",
        "\n",
        "def build_model(patch_size, batch_normalization, momentum, activation, loss, lr):\n",
        "  #Climatic features\n",
        "  input1, feature1 = inputlayers((patch_size, patch_size, 6), batch_normalization, momentum = momentum, activation = activation)\n",
        "\n",
        "  #NDVI feature\n",
        "  input2, feature2 = inputlayers((patch_size, patch_size, 1), batch_normalization, momentum = momentum, activation = activation)\n",
        "\n",
        "  #Topograhic features\n",
        "  input3, feature3 = inputlayers((patch_size, patch_size, 4), batch_normalization, momentum = momentum, activation = activation)\n",
        "\n",
        "  #Human-related features\n",
        "  input4, feature4 = inputlayers((patch_size, patch_size, 3), batch_normalization, momentum = momentum , activation = activation)\n",
        "\n",
        "  #Human-related features\n",
        "  input5, feature5 = inputlayers((patch_size, patch_size, 6), batch_normalization, momentum = momentum , activation = activation)\n",
        "\n",
        "  merge = Concatenate(axis = -1)([feature1, feature2, feature3, feature4, feature5])\n",
        "  \n",
        "  x = Conv2DTranspose(32, kernel_size=(3,3), dilation_rate=2)(merge)\n",
        "  x = Conv2D(32, kernel_size=(3, 3), activation=activation, padding = 'same')(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = Conv2D(64, kernel_size=(3, 3), activation=activation, padding = 'same')(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  conv = Conv2D(128, kernel_size=(3, 3), activation=activation, padding = 'same')(x)\n",
        "\n",
        "  flatten = Flatten()(conv)\n",
        "\n",
        "  FC2 = Dense(128, activation = activation, kernel_regularizer=regularizers.l1_l2(l1 = 0.001, l2 = 0.01))(flatten)\n",
        "  dropout2 = Dropout(0.5)(FC2)\n",
        "  FC3 = Dense(64, activation = activation, kernel_regularizer=regularizers.l1_l2(l1 = 0.001, l2 = 0.01))(dropout2)\n",
        "  dropout3 = Dropout(0.5)(FC3)\n",
        "  FC4 = Dense(32, activation = activation, kernel_regularizer=regularizers.l1_l2(l1 = 0.001, l2 = 0.01))(dropout3)\n",
        "  dropout4 = Dropout(0.5)(FC4)\n",
        "  if loss == 'binary_crossentropy':\n",
        "    output = Dense(1, activation = 'sigmoid')(dropout4)\n",
        "  elif loss == 'sparse_categorical_crossentropy':\n",
        "    output = Dense(2, activation = 'softmax')(dropout4)  \n",
        "  \n",
        "  model = Model(inputs=[input1, input2, input3, input4, input5], outputs = output)\n",
        "  \n",
        "  model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.9999, epsilon=1e-07), metrics=['accuracy'])\n",
        "\n",
        "#kernel_regularizer=regularizers.l1_l2(l1 = 0.001,l2 = 0.01)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "bJtK-zLcLreX"
      },
      "source": [
        "#Callbacks\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=50, min_lr=0.0001, verbose=1)\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=100, min_delta=0.01, restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlicptpELQ_7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4272edd6-6bde-4048-ddcc-f5fd0cb92a5d"
      },
      "source": [
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(X, y):\n",
        "\n",
        "  # Define the model architecture\n",
        "  model = build_model(patch_size = 5, batch_normalization = True, momentum = 0.99, activation = 'relu', loss = 'binary_crossentropy', lr = 0.0001)\n",
        "\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Separate data by group\n",
        "  climatic = X[train][:,:,:,0:6]\n",
        "  vegetation = X[train][:,:,:,6:7]\n",
        "  topographic = X[train][:,:,:,7:11]\n",
        "  human_related = X[train][:,:,:,11:14]\n",
        "  LULC = X[train][:,:,:,14:21]\n",
        "\n",
        "  climatic_val = X[test][:,:,:,0:6]\n",
        "  vegetation_val = X[test][:,:,:,6:7]\n",
        "  topographic_val = X[test][:,:,:,7:11]\n",
        "  human_related_val = X[test][:,:,:,11:14]\n",
        "  LULC_val = X[test][:,:,:,14:21]\n",
        "\n",
        "  # Fit data to model\n",
        "  history = model.fit([climatic, vegetation, topographic, human_related, LULC], y[train],\n",
        "              batch_size=32,\n",
        "              epochs=200,\n",
        "              callbacks=[reduce_lr, early_stop],\n",
        "              validation_data = ([climatic_val, vegetation_val, topographic_val, human_related_val, LULC_val], y[test]), verbose = 0)\n",
        "  \n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate([climatic_val, vegetation_val, topographic_val, human_related_val, LULC_val], y[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no} on validation: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Generate generalization metrics on TRAINING\n",
        "  scores_train = model.evaluate([climatic, vegetation, topographic, human_related, LULC], y[train], verbose=0)\n",
        "  print(f'Score for fold {fold_no} on training: {model.metrics_names[0]} of {scores_train[0]}; {model.metrics_names[1]} of {scores_train[1]*100}%')\n",
        "  acc_per_fold_train.append(scores_train[1] * 100)\n",
        "  loss_per_fold_train.append(scores_train[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Dataset size:', X.shape[0])\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold on training set')\n",
        "for i in range(0, len(acc_per_fold_train)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold_train[i]} - Accuracy: {acc_per_fold_train[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold_train)} (+- {np.std(acc_per_fold_train)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold_train)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00106: early stopping\n",
            "Score for fold 1 on validation: loss of 0.6672959923744202; accuracy of 62.506675720214844%\n",
            "Score for fold 1 on training: loss of 0.6526517271995544; accuracy of 64.34770822525024%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00108: early stopping\n",
            "Score for fold 2 on validation: loss of 0.6667557954788208; accuracy of 62.23962903022766%\n",
            "Score for fold 2 on training: loss of 0.642261803150177; accuracy of 65.05096554756165%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00105: early stopping\n",
            "Score for fold 3 on validation: loss of 0.670974612236023; accuracy of 62.880539894104004%\n",
            "Score for fold 3 on training: loss of 0.6694685816764832; accuracy of 63.12369108200073%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00104: early stopping\n",
            "Score for fold 4 on validation: loss of 0.6904416084289551; accuracy of 62.57789134979248%\n",
            "Score for fold 4 on training: loss of 0.6892920136451721; accuracy of 63.17265033721924%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00105: early stopping\n",
            "Score for fold 5 on validation: loss of 0.6752029657363892; accuracy of 62.410968542099%\n",
            "Score for fold 5 on training: loss of 0.6692287921905518; accuracy of 63.445788621902466%\n",
            "------------------------------------------------------------------------\n",
            "Dataset size: 28084\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.6672959923744202 - Accuracy: 62.506675720214844%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.6667557954788208 - Accuracy: 62.23962903022766%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.670974612236023 - Accuracy: 62.880539894104004%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.6904416084289551 - Accuracy: 62.57789134979248%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.6752029657363892 - Accuracy: 62.410968542099%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 62.5231409072876 (+- 0.21164327207037217)\n",
            "> Loss: 0.6741341948509216\n",
            "------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------\n",
            "Score per fold on training set\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.6526517271995544 - Accuracy: 64.34770822525024%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.642261803150177 - Accuracy: 65.05096554756165%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.6694685816764832 - Accuracy: 63.12369108200073%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.6892920136451721 - Accuracy: 63.17265033721924%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.6692287921905518 - Accuracy: 63.445788621902466%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 63.828160762786865 (+- 0.753307508354924)\n",
            "> Loss: 0.6645805835723877\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "tMIfdyT2Lrec"
      },
      "source": [
        "history = model1.fit([climatic, vegetation, topographic, human_related, LULC], y_train, epochs=200, verbose=1, batch_size = 32,\n",
        "                    validation_data=([climatic_val, vegetation_val, topographic_val, human_related_val, LULC_val], y_test), callbacks = [early_stop, reduce_lr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhFLX03i-eUI"
      },
      "source": [
        "model1.save(\"/content/drive/My Drive/MT/checkpoints/Multi-input/Multi-input25\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1913sd2Lrei"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}