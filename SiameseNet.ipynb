{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SiameseNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNV6Wf/hhJZpsq9DXfjj+GP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alebjanes/fire-susceptibility-mapping/blob/main/SiameseNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSnupEe3-UUz"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import load\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential, Model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Activation, Input\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers.merge import concatenate\n",
        "import random\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uBharXz-Yhe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C28ZqdoY-ZLh"
      },
      "source": [
        "Dataset = np.load('/content/drive/My Drive/MT/Samples/samples5x5_v3.npy')\n",
        "\n",
        "X = Dataset[:,:,:,1:21]\n",
        "target = Dataset[:,2,2,0]\n",
        "y = np.expand_dims(target, axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iRHr9jQoHgy"
      },
      "source": [
        "X=X_train\n",
        "y=y_train\n",
        "\n",
        "del y_test\n",
        "del X_test\n",
        "del target\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhkWurBiLlFt"
      },
      "source": [
        "#CV\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=False)\n",
        "\n",
        "# Define per-fold score containers <-- these are new\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "acc_per_fold_train = []\n",
        "loss_per_fold_train = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72R_CO7zYM3q"
      },
      "source": [
        "#Create batch with (batch_size) number of pairs\n",
        "def create_batch(batch_size, x, target):\n",
        "    Left_inputs = np.zeros((batch_size, 5, 5, 20))\n",
        "    Right_inputs = np.zeros((batch_size, 5, 5, 20))\n",
        "    Label = np.zeros((batch_size,), dtype = np.int)\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "\n",
        "      random_index1 = random.randint(0, x.shape[0] - 1)\n",
        "      random_index2 = random.randint(0, x.shape[0] - 1)\n",
        "        \n",
        "      while random_index1 == random_index2:\n",
        "        random_index1 = random.randint(0, x.shape[0]-1)\n",
        "        random_index2 = random.randint(0, x.shape[0]-1)\n",
        "\n",
        "      left = x[random_index1]\n",
        "      right = x[random_index2]\n",
        "\n",
        "      #Label\n",
        "      if target[random_index1] == target[random_index2]:\n",
        "        Label[i] = int(1)\n",
        "      else: Label[i] = int(0)\n",
        "\n",
        "      Left_inputs[i] = left\n",
        "      Right_inputs[i] = right\n",
        "\n",
        "    return [Left_inputs, Right_inputs], Label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22HhaMLBOAu3"
      },
      "source": [
        "def data_generator(batch_size, Xtrain, ytrain):\n",
        "    while True:\n",
        "        x, y = create_batch(batch_size, Xtrain, ytrain)\n",
        "        yield x, y\n",
        "\n",
        "\n",
        "def testdata_generator(batch_size, X_test, y_test):\n",
        "    while True:\n",
        "        x, y = create_batch(batch_size, X_test, y_test)\n",
        "        yield x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxRX-Bw9YM1H"
      },
      "source": [
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout\n",
        "from keras.models import Sequential\n",
        "#Embedding model\n",
        "\n",
        "#encoding_size = 32\n",
        "def create_Net():\n",
        "  Net = Sequential([\n",
        "    BatchNormalization(momentum=0.99, input_shape = (5, 5, 20)),\n",
        "    Conv2D(64, 3, activation = 'relu', padding = 'same'),\n",
        "    MaxPooling2D(2),\n",
        "    Conv2D(128, 3, activation = 'relu', padding = 'same'),\n",
        "    BatchNormalization(momentum = 0.99),\n",
        "    MaxPooling2D(2),\n",
        "    Conv2D(128, 3, activation = 'relu', padding = 'same'),\n",
        "    Flatten(),\n",
        "    Dense(2048, activation='sigmoid', kernel_regularizer = tf.keras.regularizers.L1L2(l1=0.001, l2=0.01)),\n",
        "\n",
        "  ], name = 'Net')\n",
        "  return Net\n",
        "#Net.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LBlM1ZjYMyl"
      },
      "source": [
        "from keras.layers import Input, Lambda, subtract, MaxPooling2D, concatenate, Activation, Dense, Conv2D, Dropout, Flatten\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.densenet import preprocess_input\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "# create model\n",
        "def create_model(input_shape):\n",
        "  left_input = Input(input_shape, name=\"left_input\")\n",
        "  right_input = Input(input_shape, name=\"right_input\") \n",
        "\n",
        "  # create the inputs\n",
        "  enc_left = Net(left_input)\n",
        "  enc_right = Net(right_input)\n",
        "\n",
        "  Euc_layer = Lambda(lambda tensor : K.abs(tensor[0] - tensor[1]), name = 'Distance')\n",
        "\n",
        "  # use and add the distance function\n",
        "  Euc_distance = Euc_layer([enc_left, enc_right])\n",
        "\n",
        "  #identify the prediction\n",
        "  prediction = Dense(1, activation='sigmoid')(Euc_distance)\n",
        "  siamese_net = Model(inputs = [left_input, right_input], outputs = [prediction], name = 'SiameseNet')\n",
        "  \n",
        "  #MODO1\n",
        "  #output = concatenate([enc_left, enc_right], axis=1)\n",
        "  #siamese_net = Model(inputs=[left_input, right_input], outputs=output)\n",
        "  #siamese_net.summary()\n",
        "\n",
        "  siamese_net.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "        \n",
        "  return siamese_net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPa7S55CRFWK"
      },
      "source": [
        "#Callbacks\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=100, min_delta=0.01, restore_best_weights=True)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=25, min_lr=0.0001, verbose=1)\n",
        "steps = 1000\n",
        "\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(X, y):\n",
        "  \n",
        " # Define the model architecture\n",
        "  Net = create_Net()\n",
        "  model = create_model((5,5,20))\n",
        "\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  history = model.fit(data_generator(16, X[train], y[train]), \n",
        "                      validation_data = testdata_generator(16, X[test], y[test]), \n",
        "                      validation_steps = 500, \n",
        "                      epochs = 200, \n",
        "                      steps_per_epoch = steps, \n",
        "                      callbacks = [early_stop], \n",
        "                      verbose = 0)\n",
        "  \n",
        "  Net.save('/content/drive/My Drive/MT/CV_models/SN5'+ str(fold_no))\n",
        "\n",
        "  del model\n",
        "  del Net\n",
        "  K.clear_session()\n",
        "  \n",
        "\n",
        "  # Generate generalization metrics on VALIDATION\n",
        "  #scores = SN.evaluate(X[test], y[test], verbose=0)\n",
        "  #print(f'TEST: Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  #acc_per_fold.append(scores[1] * 100)\n",
        "  #loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Generate generalization metrics on TRAINING\n",
        "  #scores_train = SN.evaluate(X[train], y[train], verbose=0)\n",
        "  #print(f'TRAIN: Score for fold {fold_no}: {model.metrics_names[0]} of {scores_train[0]}; {model.metrics_names[1]} of {scores_train[1]*100}%')\n",
        "  #acc_per_fold_train.append(scores_train[1] * 100)\n",
        "  #loss_per_fold_train.append(scores_train[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9Le6kT580HN"
      },
      "source": [
        "def build_model(lr, fold_no):\n",
        "  reconstructed_model = keras.models.load_model('/content/drive/My Drive/MT/CV_models/SN5'+ str(fold_no))\n",
        "  reconstructed_model.trainable = False\n",
        "  model2 = Sequential()\n",
        "  model2.add(reconstructed_model),\n",
        "  model2.add(Dense(1, activation = 'sigmoid'))\n",
        "  \n",
        "  model2.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.9999, epsilon=1e-07), metrics=['accuracy'])\n",
        "\n",
        "  return model2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XapS75N2UMkH"
      },
      "source": [
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=100, min_delta=0.01, restore_best_weights=True)\n",
        "import keras\n",
        "\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(X, y):\n",
        "\n",
        "  # Define the model architecture\n",
        "  model = build_model(0.0001, fold_no)\n",
        "\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  history = model.fit(X[train], y[train],\n",
        "              batch_size=32,\n",
        "              epochs=200,\n",
        "              callbacks=[early_stop],\n",
        "              validation_data = (X[test], y[test]), verbose=1)\n",
        "\n",
        "\n",
        "  # Generate generalization metrics on VALIDATION\n",
        "  scores = model.evaluate(X[test], y[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Generate generalization metrics on TRAINING\n",
        "  scores_train = model.evaluate(X[train], y[train], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores_train[0]}; {model.metrics_names[1]} of {scores_train[1]*100}%')\n",
        "  acc_per_fold_train.append(scores_train[1] * 100)\n",
        "  loss_per_fold_train.append(scores_train[0])\n",
        "\n",
        "  del model\n",
        "  K.clear_session()\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold on validation set')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold on training set')\n",
        "for i in range(0, len(acc_per_fold_train)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold_train[i]} - Accuracy: {acc_per_fold_train[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold_train)} (+- {np.std(acc_per_fold_train)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold_train)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF0LWfluty--"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}