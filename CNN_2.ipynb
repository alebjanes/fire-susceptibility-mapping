{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmiF3sW5owX/KcpWJz95Dg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alebjanes/fire-susceptibility-mapping/blob/main/CNN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlTf56IClN1l"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import load\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Activation, InputLayer, BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBDBlKenlQkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e25129-a686-4ff9-e3f5-2524bae5d827"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCxL-bG9lQ4S"
      },
      "source": [
        "#DATA\n",
        "\n",
        "pixel_size = 25\n",
        "mid_pixel = pixel_size/2 - 0.5\n",
        "\n",
        "Dataset = np.load('/content/drive/My Drive/MT/Samples/samples' + str(pixel_size) + 'x'+ str(pixel_size) +'_v3.npy')\n",
        "\n",
        "X = Dataset[:,:,:,1:21]\n",
        "target = Dataset[:,mid_pixel,mid_pixel,0]\n",
        "y = np.expand_dims(target, axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcHaDELPP9CK"
      },
      "source": [
        "X=X_train\n",
        "y=y_train\n",
        "\n",
        "del y_test\n",
        "del X_test\n",
        "del target\n",
        "del X_train\n",
        "del y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbD-tdLzK_TJ"
      },
      "source": [
        "#CV\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# Define per-fold score containers <-- these are new\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "acc_per_fold_train = []\n",
        "loss_per_fold_train = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykPHwcHrlRJG"
      },
      "source": [
        "#Build CNN function (LULC paper)\n",
        "def build_model(input_shape, batch_normalization, momentum, activation, loss, lr, optimizer):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(InputLayer(input_shape = input_shape))\n",
        "    if batch_normalization: \n",
        "        model.add(BatchNormalization(momentum=momentum))\n",
        "    model.add(Conv2D(32, 3, strides = 1, padding = 'same'))\n",
        "    model.add(Activation(activation = activation))\n",
        "    if batch_normalization: \n",
        "        model.add(BatchNormalization(momentum=momentum))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "    model.add(Conv2D(64, 3, strides = 1, padding = 'same'))\n",
        "    model.add(Activation(activation = activation))\n",
        "    if batch_normalization: \n",
        "        model.add(BatchNormalization(momentum=momentum))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1024, activation = activation, kernel_regularizer=regularizers.l1_l2(l1 = 0.001, l2 = 0.01)))\n",
        "    model.add(Dropout(0.5))\n",
        "    if loss == 'binary_crossentropy':\n",
        "        model.add(Dense(1, activation = 'sigmoid'))\n",
        "    elif loss == 'sparse_categorical_crossentropy':\n",
        "        model.add(Dense(2, activation = 'softmax'))\n",
        "    \n",
        "    if optimizer == 'adam':\n",
        "      model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.9999, epsilon=1e-07), metrics=['accuracy'])\n",
        "    elif optimizer == 'sgd':\n",
        "      model.compile(loss=loss, optimizer=tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.0), metrics=['accuracy'])     \n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ_YMIXlpO7R"
      },
      "source": [
        "#Callbacks\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=10, min_lr=0.0001, verbose=1)\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=100, min_delta=0.01, restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj2nsSXRLD4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5247b646-3dc4-45e7-de4a-86271675d30c"
      },
      "source": [
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(X, y):\n",
        "\n",
        "  # Define the model architecture\n",
        "  model = build_model(input_shape = (7, 7, 20), batch_normalization=True, momentum = 0.99, activation = 'relu', loss='binary_crossentropy', lr = 0.0001, optimizer='adam')\n",
        "\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  history = model.fit(X[train], y[train],\n",
        "              batch_size=32,\n",
        "              epochs=200,\n",
        "              callbacks=[reduce_lr, early_stop],\n",
        "              validation_data = (X[test], y[test]), verbose=0)\n",
        "  \n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate(X[test], y[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no} on validation: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Generate generalization metrics on TRAINING\n",
        "  scores_train = model.evaluate(X[train], y[train], verbose=0)\n",
        "  print(f'Score for fold {fold_no} on training: {model.metrics_names[0]} of {scores_train[0]}; {model.metrics_names[1]} of {scores_train[1]*100}%')\n",
        "  acc_per_fold_train.append(scores_train[1] * 100)\n",
        "  loss_per_fold_train.append(scores_train[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold on training set')\n",
        "for i in range(0, len(acc_per_fold_train)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold_train[i]} - Accuracy: {acc_per_fold_train[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold_train)} (+- {np.std(acc_per_fold_train)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold_train)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00112: early stopping\n",
            "Score for fold 1 on validation: loss of 0.6529799699783325; accuracy of 64.26066160202026%\n",
            "Score for fold 1 on training: loss of 0.6234534978866577; accuracy of 67.20819473266602%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00104: early stopping\n",
            "Score for fold 2 on validation: loss of 0.7481210827827454; accuracy of 63.79914879798889%\n",
            "Score for fold 2 on training: loss of 0.7388977408409119; accuracy of 64.36977982521057%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00103: early stopping\n",
            "Score for fold 3 on validation: loss of 0.9030749797821045; accuracy of 63.060736656188965%\n",
            "Score for fold 3 on training: loss of 0.8973676562309265; accuracy of 63.59902024269104%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00108: early stopping\n",
            "Score for fold 4 on validation: loss of 0.6461390256881714; accuracy of 64.33449983596802%\n",
            "Score for fold 4 on training: loss of 0.6416601538658142; accuracy of 65.31130075454712%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00105: early stopping\n",
            "Score for fold 5 on validation: loss of 0.6850411891937256; accuracy of 63.441652059555054%\n",
            "Score for fold 5 on training: loss of 0.672828197479248; accuracy of 64.93446826934814%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.6529799699783325 - Accuracy: 64.26066160202026%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.7481210827827454 - Accuracy: 63.79914879798889%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.9030749797821045 - Accuracy: 63.060736656188965%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.6461390256881714 - Accuracy: 64.33449983596802%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.6850411891937256 - Accuracy: 63.441652059555054%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 63.77933979034424 (+- 0.48387792414601527)\n",
            "> Loss: 0.7270712494850159\n",
            "------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------\n",
            "Score per fold on training set\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.6234534978866577 - Accuracy: 67.20819473266602%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.7388977408409119 - Accuracy: 64.36977982521057%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.8973676562309265 - Accuracy: 63.59902024269104%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.6416601538658142 - Accuracy: 65.31130075454712%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.672828197479248 - Accuracy: 64.93446826934814%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 65.08455276489258 (+- 1.2084289340895444)\n",
            "> Loss: 0.7148414492607117\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}