{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alebjanes/fire-susceptibility-mapping/blob/main/Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMB4CQ2uFH-j"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import load\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Activation, Input, Concatenate, Conv2DTranspose\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%load_ext tensorboard\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "import logging\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im9zZPy9Fxxp"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgGTmDfmuGvG"
      },
      "source": [
        "# Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tY9G_QwF13r"
      },
      "source": [
        "patch_size = 25\n",
        "mid_pixel = int(patch_size / 2 - 0.5)\n",
        "\n",
        "Dataset = np.load('/content/drive/My Drive/MT/Samples/samples' + str(patch_size)+ 'x' + str(patch_size) + '_v3.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_B1ZXi5O2L1"
      },
      "source": [
        "X = Dataset[:,:,:,1:21]\n",
        "target = Dataset[:,mid_pixel,mid_pixel,0]\n",
        "y = np.expand_dims(target, axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQpxpuNy2ABI"
      },
      "source": [
        "X=X_train\n",
        "y=y_train\n",
        "\n",
        "del y_test\n",
        "del X_test\n",
        "del target\n",
        "del X_train\n",
        "del y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xVeWGsUOhYo"
      },
      "source": [
        "# Pre processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVboB2Fi5KXc",
        "outputId": "8e2df65c-153d-45d3-aa04-6ca056dd1150"
      },
      "source": [
        "#Elevacion capa 9 - Separacion de datos en dos grupos segun elevacion\n",
        "\n",
        "#Set values:\n",
        "threshold = 300\n",
        "minval = -0.96570312976837\n",
        "maxval = 4729.0986328125\n",
        "\n",
        "n = 0\n",
        "j = 0\n",
        "threshold_norm = (threshold - minval) / (maxval - minval)\n",
        "\n",
        "for i in range(Dataset.shape[0]):\n",
        "  patch = np.expand_dims(Dataset[i, :, :, :], axis=0)\n",
        "  elev = Dataset[i, mid_pixel, mid_pixel, 10]\n",
        "\n",
        "  if elev < threshold_norm:\n",
        "    n += 1\n",
        "    if n == 1:\n",
        "      X_class1 = patch\n",
        "      print('X_class1:', X_class1.shape)\n",
        "    else:\n",
        "      X_class1 = np.concatenate((X_class1, patch), axis=0)\n",
        "\n",
        "  else:\n",
        "    j += 1\n",
        "    if j == 1:\n",
        "      X_class2 = patch\n",
        "      print('X_class2:', X_class2.shape)\n",
        "    else:\n",
        "      X_class2 = np.concatenate((X_class2, patch), axis=0)\n",
        "\n",
        "print(\"X class 1 (< threshold):\", X_class1.shape)\n",
        "print(\"X class 2 (>= threshold):\", X_class2.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_class2: (1, 25, 25, 21)\n",
            "X_class1: (1, 25, 25, 21)\n",
            "X class 1 (< threshold): (20305, 25, 25, 21)\n",
            "X class 2 (>= threshold): (9839, 25, 25, 21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBpS2nGMPqek",
        "outputId": "33826bc1-7552-41db-b7b0-008798d28ea5"
      },
      "source": [
        "#Separacion de datos en dos grupos segun clase\n",
        "i = 0\n",
        "n = 0\n",
        "j = 0\n",
        "\n",
        "for i in range(Dataset.shape[0]):\n",
        "  patch = np.expand_dims(Dataset[i, :, :, :], axis=0)\n",
        "  clase = np.where(Dataset[i, mid_pixel, mid_pixel, 15:] == 1)\n",
        "  if clase[0].size == 0: clase = 5\n",
        "  else: clase = int(clase[0])\n",
        "\n",
        "  if clase == 0:\n",
        "    n += 1\n",
        "    if n == 1:\n",
        "      X_class1 = patch\n",
        "      print('X_class1:', X_class1.shape)\n",
        "    else:\n",
        "      X_class1 = np.concatenate((X_class1, patch), axis=0)\n",
        "\n",
        "  else:\n",
        "    j += 1\n",
        "    if j == 1:\n",
        "      X_class2 = patch\n",
        "      print('X_class2:', X_class2.shape)\n",
        "    else:\n",
        "      X_class2 = np.concatenate((X_class2, patch), axis=0)\n",
        "\n",
        "print(\"X class 1 (< threshold):\", X_class1.shape)\n",
        "print(\"X class 2 (>= threshold):\", X_class2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_class1: (1, 25, 25, 21)\n",
            "X_class2: (1, 25, 25, 21)\n",
            "X class 1 (< threshold): (15596, 25, 25, 21)\n",
            "X class 2 (>= threshold): (14548, 25, 25, 21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmTjs-Pj-g4y"
      },
      "source": [
        "np.save('/content/drive/My Drive/paper/samples/X_25_LULC_class1.npy', X_class1)\n",
        "np.save('/content/drive/My Drive/paper/samples/X_25_LULC_class2.npy', X_class2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0fd4IkctvaO"
      },
      "source": [
        "X_class1 = np.load('/content/drive/My Drive/paper/samples/X_25_LULC_class1.npy')\n",
        "X_class2 = np.load('/content/drive/My Drive/paper/samples/X_25_LULC_class2.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71Ef0KeAcrtZ"
      },
      "source": [
        "#TRAIN/TEST SPLIT\n",
        "\n",
        "X1 = X_class1[:, :, :, 1:21]\n",
        "target1 = X_class1[:, mid_pixel, mid_pixel, 0]\n",
        "y1 = np.expand_dims(target1, axis=1)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "X2 = X_class2[:, :, :, 1:21]\n",
        "target2 = X_class2[:, mid_pixel, mid_pixel, 0]\n",
        "y2 = np.expand_dims(target2, axis=1)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1UjnUf8bNEG",
        "outputId": "8e5ef92c-c50b-464c-864f-f838307d9c25"
      },
      "source": [
        "X_train2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11638, 25, 25, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEHKsOI1eiwR",
        "outputId": "9f8ebfd9-095b-4340-e9ed-2162c7f4582c"
      },
      "source": [
        "print('Clase verde -----------------------\\n')\n",
        "\n",
        "print('Total muestras:', X_class1.shape[0])\n",
        "print('Total muestras de incendios:', np.sum(y_train1) + np.sum(y_test1))\n",
        "print('Total de muestras de no incendio:', X_class1.shape[0] - (np.sum(y_train1) + np.sum(y_test1)), '\\n' )\n",
        "\n",
        "print('Set de validacion \\n')\n",
        "\n",
        "print('ones:', np.sum(y_test1))\n",
        "print('zeros:', X_test1.shape[0] - np.sum(y_test1))\n",
        "\n",
        "print('\\n')\n",
        "print('Otras clases -----------------------\\n')\n",
        "\n",
        "print('Total muestras:', X_class2.shape[0])\n",
        "print('Total muestras de incendios:', np.sum(y_train2) + np.sum(y_test2))\n",
        "print('Total de muestras de no incendio:', X_class2.shape[0] - (np.sum(y_train2) + np.sum(y_test2)), '\\n' )\n",
        "\n",
        "print('Set de validacion \\n')\n",
        "\n",
        "print('ones:', np.sum(y_test2))\n",
        "print('zeros:', X_test2.shape[0] - np.sum(y_test2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clase verde -----------------------\n",
            "\n",
            "Total muestras: 15596\n",
            "Total muestras de incendios: 6892.0\n",
            "Total de muestras de no incendio: 8704.0 \n",
            "\n",
            "Set de validacion \n",
            "\n",
            "ones: 1397.0\n",
            "zeros: 1723.0\n",
            "\n",
            "\n",
            "Otras clases -----------------------\n",
            "\n",
            "Total muestras: 14548\n",
            "Total muestras de incendios: 8180.0\n",
            "Total de muestras de no incendio: 6368.0 \n",
            "\n",
            "Set de validacion \n",
            "\n",
            "ones: 1653.0\n",
            "zeros: 1257.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW9NWQ-3uQ5K"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7-qLiM-L824"
      },
      "source": [
        "CNN1 = tf.keras.models.load_model('/content/drive/My Drive/MT/checkpoints/CNN1/CNN1_25')\n",
        "\n",
        "y_pred_CNN1 = CNN1.predict(X_test).ravel()\n",
        "\n",
        "y_pred_CNN1_classes = np.where(y_pred_CNN1 > 0.5, 1, 0)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "CNN1.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVnznxWvGBwA"
      },
      "source": [
        "#MODELS\n",
        "\n",
        "def cnn1(input_shape, batch_normalization, momentum, activation, loss, lr, optimizer):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, 3, strides = 1, padding = 'same', input_shape = input_shape))\n",
        "    model.add(Activation(activation = activation))\n",
        "    if batch_normalization: \n",
        "        model.add(BatchNormalization(momentum=momentum, scale = False, renorm = True))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "    model.add(Conv2D(64, 3, strides = 1, padding = 'same'))\n",
        "    model.add(Activation(activation = activation))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "    model.add(Conv2D(128, 3, strides = 1, padding = 'same'))\n",
        "    model.add(Activation(activation = activation))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation = activation))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation = activation))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(32, activation  = activation))\n",
        "    model.add(Dropout(0.5))\n",
        "    if loss == 'binary_crossentropy':\n",
        "        model.add(Dense(1, activation = 'sigmoid'))\n",
        "    elif loss == 'sparse_categorical_crossentropy':\n",
        "        model.add(Dense(2, activation = 'softmax'))  \n",
        "\n",
        "    if optimizer == 'adam':\n",
        "      model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.9999, epsilon=1e-07), metrics=['accuracy' \n",
        "                                                                                                                                        #, tf.keras.metrics.AUC()\n",
        "                                                                                                                                        ])\n",
        "    elif optimizer == 'sgd':\n",
        "      model.compile(loss=loss, optimizer=tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9), metrics=['accuracy'])   \n",
        "\n",
        "    #model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "def inputlayers(input_shape, batch_normalization, momentum, activation):\n",
        "  input = Input(shape=input_shape)\n",
        "  x = Conv2D(32, kernel_size=(3, 3), activation=activation, padding = 'same')(input)\n",
        "  if batch_normalization: \n",
        "    x = BatchNormalization(momentum=0.99)(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = Conv2D(64, kernel_size=(3, 3), activation=activation, padding = 'same')(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  conv = Conv2D(128, kernel_size=(3, 3), activation=activation, padding = 'same')(x)\n",
        "\n",
        "  return input, conv\n",
        "\n",
        "def multi_input(patch_size, batch_normalization, momentum, activation, loss, lr):\n",
        "  #Climatic features\n",
        "  input1, feature1 = inputlayers((patch_size, patch_size, 6), batch_normalization, momentum = momentum, activation = activation)\n",
        "\n",
        "  #NDVI feature\n",
        "  input2, feature2 = inputlayers((patch_size, patch_size, 1), batch_normalization, momentum = momentum, activation = activation)\n",
        "\n",
        "  #Topograhic features\n",
        "  input3, feature3 = inputlayers((patch_size, patch_size, 4), batch_normalization, momentum = momentum, activation = activation)\n",
        "\n",
        "  #Human-related features\n",
        "  input4, feature4 = inputlayers((patch_size, patch_size, 3), batch_normalization, momentum = momentum , activation = activation)\n",
        "\n",
        "  #Human-related features\n",
        "  input5, feature5 = inputlayers((patch_size, patch_size, 6), batch_normalization, momentum = momentum , activation = activation)\n",
        "\n",
        "  merge = Concatenate(axis = -1)([feature1, feature2, feature3, feature4, feature5])\n",
        "  \n",
        "  x = Conv2DTranspose(32, kernel_size=(3,3), dilation_rate=2)(merge)\n",
        "  x = Conv2D(32, kernel_size=(3, 3), activation=activation, padding = 'same')(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = Conv2D(64, kernel_size=(3, 3), activation=activation, padding = 'same')(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  conv = Conv2D(128, kernel_size=(3, 3), activation=activation, padding = 'same')(x)\n",
        "\n",
        "  flatten = Flatten()(conv)\n",
        "\n",
        "  FC2 = Dense(128, activation = activation, kernel_regularizer=regularizers.l1_l2(l1 = 0.001, l2 = 0.01))(flatten)\n",
        "  dropout2 = Dropout(0.5)(FC2)\n",
        "  FC3 = Dense(64, activation = activation, kernel_regularizer=regularizers.l1_l2(l1 = 0.001, l2 = 0.01))(dropout2)\n",
        "  dropout3 = Dropout(0.5)(FC3)\n",
        "  FC4 = Dense(32, activation = activation, kernel_regularizer=regularizers.l1_l2(l1 = 0.001, l2 = 0.01))(dropout3)\n",
        "  dropout4 = Dropout(0.5)(FC4)\n",
        "  if loss == 'binary_crossentropy':\n",
        "    output = Dense(1, activation = 'sigmoid')(dropout4)\n",
        "  elif loss == 'sparse_categorical_crossentropy':\n",
        "    output = Dense(2, activation = 'softmax')(dropout4)  \n",
        "  \n",
        "  model = Model(inputs=[input1, input2, input3, input4, input5], outputs = output)\n",
        "  \n",
        "  model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.9999, epsilon=1e-07), metrics=['accuracy'])\n",
        "\n",
        "#kernel_regularizer=regularizers.l1_l2(l1 = 0.001,l2 = 0.01)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXADtoEiGCFG"
      },
      "source": [
        "#Callbacks\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=15, min_lr=0.0001, verbose=1)\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=200, min_delta=0.01, restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAnYFbJGdwxg"
      },
      "source": [
        "#CNN! XCLASS 1\n",
        "model1 = cnn1(input_shape = (25, 25, 20), batch_normalization=True, momentum = 0.99, activation = 'relu', loss='binary_crossentropy', lr = 0.0001, optimizer = 'adam')\n",
        "\n",
        "history1 = model1.fit(X_train1, y_train1, batch_size = 32, epochs = 500, callbacks = [reduce_lr, early_stop], validation_data = (X_test1, y_test1), verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDrATSEPglTC"
      },
      "source": [
        "model2 = cnn1(input_shape = (25, 25, 20), batch_normalization=True, momentum = 0.99, activation = 'relu', loss='binary_crossentropy', lr = 0.0001, optimizer = 'adam')\n",
        "\n",
        "history2 = model2.fit(X_train2, y_train2, batch_size = 32, epochs = 300, callbacks = [reduce_lr, early_stop], validation_data = (X_test2, y_test2), verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MuML3FYF0KV"
      },
      "source": [
        "#def data_separator(batch_size, X, y):\n",
        "#  while True:\n",
        "\n",
        "#    climatic = X[:,:,:,0:6]\n",
        "#    vegetation = X[:,:,:,6:7]\n",
        "#    topographic = X[:,:,:,7:11]\n",
        "#    human_related = X[:,:,:,11:14]\n",
        "#    LULC = X[:,:,:,14:21]\n",
        "    \n",
        "#    yield"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZAMae15Ltq3"
      },
      "source": [
        "climatic1 = X_train1[:,:,:,0:6]\n",
        "vegetation1 = X_train1[:,:,:,6:7]\n",
        "topographic1 = X_train1[:,:,:,7:11]\n",
        "human_related1 = X_train1[:,:,:,11:14]\n",
        "LULC1 = X_train1[:,:,:,14:21]\n",
        "\n",
        "climatic_val1 = X_test1[:,:,:,0:6]\n",
        "vegetation_val1 = X_test1[:,:,:,6:7]\n",
        "topographic_val1 = X_test1[:,:,:,7:11]\n",
        "human_related_val1 = X_test1[:,:,:,11:14]\n",
        "LULC_val1 = X_test1[:,:,:,14:21]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4XrYvhjF0Ak"
      },
      "source": [
        "MI_1 = multi_input(patch_size = 25, batch_normalization = True, momentum = 0.99, activation = 'relu', loss = 'binary_crossentropy', lr = 0.0001)\n",
        "\n",
        "history = MI_1.fit([climatic1, vegetation1, topographic1, human_related1, LULC1], y_train1,\n",
        "              batch_size=32,\n",
        "              epochs=5000,\n",
        "              callbacks=[reduce_lr],\n",
        "              validation_data = ([climatic_val1, vegetation_val1, topographic_val1, human_related_val1, LULC_val1], y_test1), verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K5qrjxuhYIW"
      },
      "source": [
        "#modelcnn.save('/content/drive/My Drive/MT/checkpoints/CNN1/CNN1_')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtevC8ErUp57",
        "outputId": "4e82af47-022a-495e-845f-26321da697fd"
      },
      "source": [
        "MI_1.save('/content/drive/My Drive/MT/paper/models/MI_1_LULC')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/MT/paper/models/MI_1_LULC/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHGnvn--Hna2"
      },
      "source": [
        "climatic2 = X_train2[:,:,:,0:6]\n",
        "vegetation2 = X_train2[:,:,:,6:7]\n",
        "topographic2 = X_train2[:,:,:,7:11]\n",
        "human_related2 = X_train2[:,:,:,11:14]\n",
        "LULC2 = X_train2[:,:,:,14:21]\n",
        "\n",
        "climatic_val2 = X_test2[:,:,:,0:6]\n",
        "vegetation_val2 = X_test2[:,:,:,6:7]\n",
        "topographic_val2 = X_test2[:,:,:,7:11]\n",
        "human_related_val2 = X_test2[:,:,:,11:14]\n",
        "LULC_val2 = X_test2[:,:,:,14:21]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o08asfyNN2oX"
      },
      "source": [
        "MI_2 = multi_input(patch_size = 25, batch_normalization = True, momentum = 0.99, activation = 'relu', loss = 'binary_crossentropy', lr = 0.0001)\n",
        "\n",
        "history2 = MI_2.fit([climatic2, vegetation2, topographic2, human_related2, LULC2], y_train2,\n",
        "              batch_size=32,\n",
        "              epochs=300,\n",
        "              callbacks=[reduce_lr, early_stop],\n",
        "              validation_data = ([climatic_val2, vegetation_val2, topographic_val2, human_related_val2, LULC_val2], y_test2), verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypV_9hQkug_9"
      },
      "source": [
        "# CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knK_AL3fQmwF",
        "outputId": "e83d7637-ef46-4ea3-971c-031007f6db4a"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28084, 5, 5, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOr7K4203v0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78e311d-5ab9-4ad2-b346-f2295f77e9e2"
      },
      "source": [
        "#CV\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=False, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIKrsCKgxewF",
        "outputId": "c1eddd39-bb61-4bdd-fbd4-944ca20eafd0"
      },
      "source": [
        "fold_no = 1\n",
        "for train, test in kfold.split(X, y):\n",
        "  \n",
        "  n = 0\n",
        "  j = 0 \n",
        "\n",
        "  for i in range(X[train].shape[0]):\n",
        "    patch = np.expand_dims(X[train][i, :, :, :], axis=0)\n",
        "\n",
        "    if X[train][i, mid_pixel, mid_pixel, 14] == 1:\n",
        "      n += 1\n",
        "      if n == 1:\n",
        "        X_train1 = patch\n",
        "        y_train1 = y[train][i]\n",
        "      else: \n",
        "        X_train1 = np.concatenate((X_train1, patch), axis = 0)\n",
        "        y_train1 = np.concatenate((y_train1, y[train][i]), axis = 0)\n",
        "\n",
        "    else:\n",
        "      j += 1\n",
        "      if j == 1:\n",
        "        X_train2 = patch\n",
        "        y_train2 = y[train][i]\n",
        "      else:\n",
        "        X_train2 = np.concatenate((X_train2, patch), axis=0)\n",
        "        y_train2 = np.concatenate((y_train2, y[train][i]), axis = 0)\n",
        "\n",
        "    if i == 100 or i == 500 or i == 1000 or i == 2000 or i == 10000 or i == 15000:    \n",
        "      print('Clase 0:', n, \"//// Otras clases:\", j)\n",
        "\n",
        "\n",
        "  np.save('/content/drive/My Drive/paper/samples/kfold/X_train1_fold' + str(fold_no) + '_7.npy', X_train1)\n",
        "  np.save('/content/drive/My Drive/paper/samples/kfold/X_train2_fold' + str(fold_no) + '_7.npy', X_train2)\n",
        "  np.save('/content/drive/My Drive/paper/samples/kfold/y_train1_fold' + str(fold_no) + '_7.npy', y_train1)\n",
        "  np.save('/content/drive/My Drive/paper/samples/kfold/y_train2_fold' + str(fold_no) + '_7.npy', y_train2)\n",
        "  \n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "  print(fold_no)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clase 0: 58 //// Otras clases: 43\n",
            "Clase 0: 259 //// Otras clases: 242\n",
            "Clase 0: 516 //// Otras clases: 485\n",
            "Clase 0: 995 //// Otras clases: 1006\n",
            "Clase 0: 5072 //// Otras clases: 4929\n",
            "Clase 0: 7665 //// Otras clases: 7336\n",
            "2\n",
            "Clase 0: 54 //// Otras clases: 47\n",
            "Clase 0: 265 //// Otras clases: 236\n",
            "Clase 0: 521 //// Otras clases: 480\n",
            "Clase 0: 1032 //// Otras clases: 969\n",
            "Clase 0: 5136 //// Otras clases: 4865\n",
            "Clase 0: 7729 //// Otras clases: 7272\n",
            "3\n",
            "Clase 0: 54 //// Otras clases: 47\n",
            "Clase 0: 265 //// Otras clases: 236\n",
            "Clase 0: 521 //// Otras clases: 480\n",
            "Clase 0: 1032 //// Otras clases: 969\n",
            "Clase 0: 5098 //// Otras clases: 4903\n",
            "Clase 0: 7669 //// Otras clases: 7332\n",
            "4\n",
            "Clase 0: 54 //// Otras clases: 47\n",
            "Clase 0: 265 //// Otras clases: 236\n",
            "Clase 0: 521 //// Otras clases: 480\n",
            "Clase 0: 1032 //// Otras clases: 969\n",
            "Clase 0: 5098 //// Otras clases: 4903\n",
            "Clase 0: 7649 //// Otras clases: 7352\n",
            "5\n",
            "Clase 0: 54 //// Otras clases: 47\n",
            "Clase 0: 265 //// Otras clases: 236\n",
            "Clase 0: 521 //// Otras clases: 480\n",
            "Clase 0: 1032 //// Otras clases: 969\n",
            "Clase 0: 5098 //// Otras clases: 4903\n",
            "Clase 0: 7649 //// Otras clases: 7352\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUEp_wBBQAC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99e14f95-96f1-408e-803f-e078fbd634ff"
      },
      "source": [
        "fold_no = 1\n",
        "for train, test in kfold.split(X, y):\n",
        "  \n",
        "  n = 0\n",
        "  j = 0 \n",
        "\n",
        "  for i in range(X[test].shape[0]):\n",
        "    patch = np.expand_dims(X[test][i, :, :, :], axis=0)\n",
        "\n",
        "    if X[test][i, mid_pixel, mid_pixel, 14] == 1:\n",
        "      n += 1\n",
        "      if n == 1:\n",
        "        X_test1 = patch\n",
        "        y_test1 = y[test][i]\n",
        "      else: \n",
        "        X_test1 = np.concatenate((X_test1, patch), axis = 0)\n",
        "        y_test1 = np.concatenate((y_test1, y[test][i]), axis = 0)\n",
        "\n",
        "    else:\n",
        "      j += 1\n",
        "      if j == 1:\n",
        "        X_test2 = patch\n",
        "        y_test2 = y[test][i]\n",
        "      else:\n",
        "        X_test2 = np.concatenate((X_test2, patch), axis=0)\n",
        "        y_test2 = np.concatenate((y_test2, y[test][i]), axis = 0)\n",
        "\n",
        "    if i == 100 or i == 500 or i == 1000 or i == 2000 or i == 10000 or i == 15000:    \n",
        "      print('Clase 0:', n, \"//// Otras clases:\", j)\n",
        "\n",
        "\n",
        "  np.save('/content/drive/My Drive/paper/samples/kfold/X_test1_fold' + str(fold_no) + '_7.npy', X_test1)\n",
        "  np.save('/content/drive/My Drive/paper/samples/kfold/X_test2_fold' + str(fold_no) + '_7.npy', X_test2)\n",
        "  np.save('/content/drive/My Drive/paper/samples/kfold/y_test1_fold' + str(fold_no) + '_7.npy', y_test1)\n",
        "  np.save('/content/drive/My Drive/paper/samples/kfold/y_test2_fold' + str(fold_no) + '_7.npy', y_test2)\n",
        "  \n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "  print(fold_no)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clase 0: 54 //// Otras clases: 47\n",
            "Clase 0: 265 //// Otras clases: 236\n",
            "Clase 0: 521 //// Otras clases: 480\n",
            "Clase 0: 1032 //// Otras clases: 969\n",
            "2\n",
            "Clase 0: 58 //// Otras clases: 43\n",
            "Clase 0: 259 //// Otras clases: 242\n",
            "Clase 0: 516 //// Otras clases: 485\n",
            "Clase 0: 995 //// Otras clases: 1006\n",
            "3\n",
            "Clase 0: 51 //// Otras clases: 50\n",
            "Clase 0: 267 //// Otras clases: 234\n",
            "Clase 0: 509 //// Otras clases: 492\n",
            "Clase 0: 1015 //// Otras clases: 986\n",
            "4\n",
            "Clase 0: 55 //// Otras clases: 46\n",
            "Clase 0: 260 //// Otras clases: 241\n",
            "Clase 0: 530 //// Otras clases: 471\n",
            "Clase 0: 1034 //// Otras clases: 967\n",
            "5\n",
            "Clase 0: 63 //// Otras clases: 38\n",
            "Clase 0: 268 //// Otras clases: 233\n",
            "Clase 0: 549 //// Otras clases: 452\n",
            "Clase 0: 1073 //// Otras clases: 928\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy5KaXsFbKke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef85b0c-343b-4d3f-ea5a-24b3ab3d047f"
      },
      "source": [
        "acc_per_fold_1 = []\n",
        "loss_per_fold_1 = []\n",
        "\n",
        "acc_per_fold_train_1 = []\n",
        "loss_per_fold_train_1 = []\n",
        "\n",
        "# K-fold Cross Validation model evaluation MULTI INPUT\n",
        "fold_no = 1\n",
        "for fold_no in range(1, 6):\n",
        "\n",
        "  #LOAD FOLD DATASET\n",
        "  X_test1 = np.load('/content/drive/My Drive/paper/samples/kfold/X_test1_fold' + str(fold_no) + '_7.npy')\n",
        "  y_test1 = np.load('/content/drive/My Drive/paper/samples/kfold/y_test1_fold' + str(fold_no) + '_7.npy')\n",
        "\n",
        "  X_train1 = np.load('/content/drive/My Drive/paper/samples/kfold/X_train1_fold' + str(fold_no) + '_7.npy')\n",
        "  y_train1 = np.load('/content/drive/My Drive/paper/samples/kfold/y_train1_fold' + str(fold_no) + '_7.npy')\n",
        "\n",
        "  #Multi input partition\n",
        "\n",
        "  climatic1 = X_train1[:,:,:,0:6]\n",
        "  vegetation1 = X_train1[:,:,:,6:7]\n",
        "  topographic1 = X_train1[:,:,:,7:11]\n",
        "  human_related1 = X_train1[:,:,:,11:14]\n",
        "  LULC1 = X_train1[:,:,:,14:21]\n",
        "\n",
        "  climatic_val1 = X_test1[:,:,:,0:6]\n",
        "  vegetation_val1 = X_test1[:,:,:,6:7]\n",
        "  topographic_val1 = X_test1[:,:,:,7:11]\n",
        "  human_related_val1 = X_test1[:,:,:,11:14]\n",
        "  LULC_val1 = X_test1[:,:,:,14:21]\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} / Multi input / GROUP 1...')\n",
        "\n",
        "  MI = multi_input(patch_size = 7, batch_normalization = True, momentum = 0.99, activation = 'relu', loss = 'binary_crossentropy', lr = 0.0001)\n",
        "\n",
        "  history_MI1 = MI.fit([climatic1, vegetation1, topographic1, human_related1, LULC1], y_train1,\n",
        "              batch_size=32,\n",
        "              epochs=5000,\n",
        "              callbacks=[reduce_lr, early_stop],\n",
        "              validation_data = ([climatic_val1, vegetation_val1, topographic_val1, human_related_val1, LULC_val1], y_test1), verbose = 0)\n",
        "  \n",
        "  # Generate generalization metrics on VALIDATION\n",
        "  scores = MI.evaluate([climatic_val1, vegetation_val1, topographic_val1, human_related_val1, LULC_val1], y_test1, verbose=0)\n",
        "  print(f'Score for fold {fold_no} on TESTING: {MI.metrics_names[0]} of {scores[0]}; {MI.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold_1.append(scores[1] * 100)\n",
        "  loss_per_fold_1.append(scores[0])\n",
        "\n",
        "  # Generate generalization metrics on TRAINING\n",
        "  scores_train = MI.evaluate([climatic1, vegetation1, topographic1, human_related1, LULC1], y_train1, verbose=0)\n",
        "  print(f'Score for fold {fold_no} on TRAINING: {MI.metrics_names[0]} of {scores_train[0]}; {MI.metrics_names[1]} of {scores_train[1]*100}%')\n",
        "  acc_per_fold_train_1.append(scores_train[1] * 100)\n",
        "  loss_per_fold_train_1.append(scores_train[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold on validation set')\n",
        "for i in range(0, len(acc_per_fold_1)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold_1[i]} - Accuracy: {acc_per_fold_1[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold_1)} (+- {np.std(acc_per_fold_1)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold_1)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold on training set')\n",
        "for i in range(0, len(acc_per_fold_train_1)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold_train_1[i]} - Accuracy: {acc_per_fold_train_1[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold_train_1)} (+- {np.std(acc_per_fold_train_1)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold_train_1)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 / Multi input / GROUP 1...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00359: early stopping\n",
            "Score for fold 1 on TESTING: loss of 1.5248509645462036; accuracy of 67.86355376243591%\n",
            "Score for fold 1 on TRAINING: loss of 0.24959230422973633; accuracy of 89.21753168106079%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 / Multi input / GROUP 1...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00210: early stopping\n",
            "Score for fold 2 on TESTING: loss of 0.6608866453170776; accuracy of 66.29915237426758%\n",
            "Score for fold 2 on TRAINING: loss of 0.6282060742378235; accuracy of 70.7156240940094%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 / Multi input / GROUP 1...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00208: early stopping\n",
            "Score for fold 3 on TESTING: loss of 0.691788911819458; accuracy of 65.98345637321472%\n",
            "Score for fold 3 on TRAINING: loss of 0.6529460549354553; accuracy of 69.88621354103088%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 / Multi input / GROUP 1...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00208: early stopping\n",
            "Score for fold 4 on TESTING: loss of 0.6935794353485107; accuracy of 66.51262044906616%\n",
            "Score for fold 4 on TRAINING: loss of 0.6699561476707458; accuracy of 68.95498037338257%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 / Multi input / GROUP 1...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00259: early stopping\n",
            "Score for fold 5 on TESTING: loss of 1.1037763357162476; accuracy of 66.18578433990479%\n",
            "Score for fold 5 on TRAINING: loss of 0.44278356432914734; accuracy of 81.32432699203491%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold on validation set\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 1.5248509645462036 - Accuracy: 67.86355376243591%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.6608866453170776 - Accuracy: 66.29915237426758%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.691788911819458 - Accuracy: 65.98345637321472%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.6935794353485107 - Accuracy: 66.51262044906616%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 1.1037763357162476 - Accuracy: 66.18578433990479%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 66.56891345977783 (+- 0.6695642437106275)\n",
            "> Loss: 0.9349764585494995\n",
            "------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------\n",
            "Score per fold on training set\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.24959230422973633 - Accuracy: 89.21753168106079%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.6282060742378235 - Accuracy: 70.7156240940094%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.6529460549354553 - Accuracy: 69.88621354103088%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.6699561476707458 - Accuracy: 68.95498037338257%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.44278356432914734 - Accuracy: 81.32432699203491%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 76.01973533630371 (+- 7.9747723398423735)\n",
            "> Loss: 0.5286968290805817\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kexZzB3zLNJe",
        "outputId": "7962b653-9e6b-4a38-bdea-5347090742c0"
      },
      "source": [
        "acc_per_fold_2 = []\n",
        "loss_per_fold_2 = []\n",
        "\n",
        "acc_per_fold_train_2 = []\n",
        "loss_per_fold_train_2 = []\n",
        "\n",
        "# K-fold Cross Validation model evaluation MULTI INPUT\n",
        "fold_no = 1\n",
        "for fold_no in range(1, 6):\n",
        "\n",
        "  #LOAD FOLD DATASET\n",
        "  X_test2 = np.load('/content/drive/My Drive/paper/samples/kfold/X_test2_fold' + str(fold_no) + '_7.npy')\n",
        "  y_test2 = np.load('/content/drive/My Drive/paper/samples/kfold/y_test2_fold' + str(fold_no) + '_7.npy')\n",
        "\n",
        "  X_train2 = np.load('/content/drive/My Drive/paper/samples/kfold/X_train2_fold' + str(fold_no) + '_7.npy')\n",
        "  y_train2 = np.load('/content/drive/My Drive/paper/samples/kfold/y_train2_fold' + str(fold_no) + '_7.npy')\n",
        "\n",
        "  #Multi input partition\n",
        "\n",
        "  climatic2 = X_train2[:,:,:,0:6]\n",
        "  vegetation2 = X_train2[:,:,:,6:7]\n",
        "  topographic2 = X_train2[:,:,:,7:11]\n",
        "  human_related2 = X_train2[:,:,:,11:14]\n",
        "  LULC2 = X_train2[:,:,:,14:21]\n",
        "\n",
        "  climatic_val2 = X_test2[:,:,:,0:6]\n",
        "  vegetation_val2 = X_test2[:,:,:,6:7]\n",
        "  topographic_val2 = X_test2[:,:,:,7:11]\n",
        "  human_related_val2 = X_test2[:,:,:,11:14]\n",
        "  LULC_val2 = X_test2[:,:,:,14:21]\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} / Multi input / GROUP 2...')\n",
        "\n",
        "  MI2 = multi_input(patch_size = 7, batch_normalization = True, momentum = 0.99, activation = 'relu', loss = 'binary_crossentropy', lr = 0.0001)\n",
        "\n",
        "  history_MI2 = MI2.fit([climatic2, vegetation2, topographic2, human_related2, LULC2], y_train2,\n",
        "              batch_size=32,\n",
        "              epochs=300,\n",
        "              callbacks=[reduce_lr, early_stop],\n",
        "              validation_data = ([climatic_val2, vegetation_val2, topographic_val2, human_related_val2, LULC_val2], y_test2), verbose = 0)\n",
        "  \n",
        "  # Generate generalization metrics on VALIDATION\n",
        "  scores = MI2.evaluate([climatic_val2, vegetation_val2, topographic_val2, human_related_val2, LULC_val2], y_test2, verbose=0)\n",
        "  print(f'Score for fold {fold_no} on TESTING: {MI2.metrics_names[0]} of {scores[0]}; {MI2.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold_2.append(scores[1] * 100)\n",
        "  loss_per_fold_2.append(scores[0])\n",
        "\n",
        "  # Generate generalization metrics on TRAINING\n",
        "  scores_train = MI2.evaluate([climatic2, vegetation2, topographic2, human_related2, LULC2], y_train2, verbose=0)\n",
        "  print(f'Score for fold {fold_no} on TRAINING: {MI2.metrics_names[0]} of {scores_train[0]}; {MI2.metrics_names[1]} of {scores_train[1]*100}%')\n",
        "  acc_per_fold_train_2.append(scores_train[1] * 100)\n",
        "  loss_per_fold_train_2.append(scores_train[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold on validation set')\n",
        "for i in range(0, len(acc_per_fold_2)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold_2[i]} - Accuracy: {acc_per_fold_2[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold_2)} (+- {np.std(acc_per_fold_2)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold_2)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold on training set')\n",
        "for i in range(0, len(acc_per_fold_train_2)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold_train_2[i]} - Accuracy: {acc_per_fold_train_2[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold_train_2)} (+- {np.std(acc_per_fold_train_2)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold_train_2)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 / Multi input / GROUP 2...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00206: early stopping\n",
            "Score for fold 1 on TESTING: loss of 0.8162353038787842; accuracy of 60.9422504901886%\n",
            "Score for fold 1 on TRAINING: loss of 0.7902636528015137; accuracy of 64.27212357521057%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 / Multi input / GROUP 2...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00207: early stopping\n",
            "Score for fold 2 on TESTING: loss of 0.7323052287101746; accuracy of 61.83234453201294%\n",
            "Score for fold 2 on TRAINING: loss of 0.7181349396705627; accuracy of 63.63201141357422%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 / Multi input / GROUP 2...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00207: early stopping\n",
            "Score for fold 3 on TESTING: loss of 0.7316224575042725; accuracy of 63.01214098930359%\n",
            "Score for fold 3 on TRAINING: loss of 0.7220139503479004; accuracy of 64.3061101436615%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 / Multi input / GROUP 2...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00207: early stopping\n",
            "Score for fold 4 on TESTING: loss of 0.7556759119033813; accuracy of 63.2488489151001%\n",
            "Score for fold 4 on TRAINING: loss of 0.7087175846099854; accuracy of 63.99696469306946%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 / Multi input / GROUP 2...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00242: early stopping\n",
            "Score for fold 5 on TESTING: loss of 1.2480779886245728; accuracy of 62.08236217498779%\n",
            "Score for fold 5 on TRAINING: loss of 0.47943973541259766; accuracy of 77.07229256629944%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold on validation set\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.8162353038787842 - Accuracy: 60.9422504901886%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.7323052287101746 - Accuracy: 61.83234453201294%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.7316224575042725 - Accuracy: 63.01214098930359%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.7556759119033813 - Accuracy: 63.2488489151001%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 1.2480779886245728 - Accuracy: 62.08236217498779%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 62.2235894203186 (+- 0.8352027011335671)\n",
            "> Loss: 0.8567833781242371\n",
            "------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------\n",
            "Score per fold on training set\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.7902636528015137 - Accuracy: 64.27212357521057%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.7181349396705627 - Accuracy: 63.63201141357422%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.7220139503479004 - Accuracy: 64.3061101436615%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.7087175846099854 - Accuracy: 63.99696469306946%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.47943973541259766 - Accuracy: 77.07229256629944%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 66.65590047836304 (+- 5.213808121846222)\n",
            "> Loss: 0.683713972568512\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oQEVM45VMHZ",
        "outputId": "a262653b-c983-4cf8-ab6b-970b1fb515a5"
      },
      "source": [
        "acc_per_fold_1 = []\n",
        "loss_per_fold_1 = []\n",
        "\n",
        "acc_per_fold_train_1 = []\n",
        "loss_per_fold_train_1 = []\n",
        "\n",
        "# K-fold Cross Validation model evaluation CNN1\n",
        "fold_no = 1\n",
        "for fold_no in range(1, 6):\n",
        "\n",
        "  #LOAD FOLD DATASET\n",
        "  X_test1 = np.load('/content/drive/My Drive/paper/samples/kfold/X_test1_fold' + str(fold_no) + '_7.npy')\n",
        "  y_test1 = np.load('/content/drive/My Drive/paper/samples/kfold/y_test1_fold' + str(fold_no) + '_7.npy')\n",
        "\n",
        "  X_train1 = np.load('/content/drive/My Drive/paper/samples/kfold/X_train1_fold' + str(fold_no) + '_7.npy')\n",
        "  y_train1 = np.load('/content/drive/My Drive/paper/samples/kfold/y_train1_fold' + str(fold_no) + '_7.npy')\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} / CNN1 / GROUP 1...')\n",
        "\n",
        "  CNN1 = cnn1(input_shape = (7, 7, 20), batch_normalization=True, momentum = 0.99, activation = 'relu', loss='binary_crossentropy', lr = 0.0001, optimizer = 'adam')\n",
        "\n",
        "  history_CNN1 = CNN1.fit(X_train1, y_train1, batch_size = 32, epochs = 500, callbacks = [reduce_lr, early_stop], validation_data = (X_test1, y_test1), verbose = 0)\n",
        "  \n",
        "  # Generate generalization metrics on VALIDATION\n",
        "  scores = CNN1.evaluate(X_test1, y_test1, verbose=0)\n",
        "  print(f'Score for fold {fold_no} on TESTING: {CNN1.metrics_names[0]} of {scores[0]}; {CNN1.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold_1.append(scores[1] * 100)\n",
        "  loss_per_fold_1.append(scores[0])\n",
        "\n",
        "  # Generate generalization metrics on TRAINING\n",
        "  scores_train = CNN1.evaluate(X_train1, y_train1, verbose=0)\n",
        "  print(f'Score for fold {fold_no} on TRAINING: {CNN1.metrics_names[0]} of {scores_train[0]}; {CNN1.metrics_names[1]} of {scores_train[1]*100}%')\n",
        "  acc_per_fold_train_1.append(scores_train[1] * 100)\n",
        "  loss_per_fold_train_1.append(scores_train[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold on validation set')\n",
        "for i in range(0, len(acc_per_fold_1)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold_1[i]} - Accuracy: {acc_per_fold_1[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold_1)} (+- {np.std(acc_per_fold_1)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold_1)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold on training set')\n",
        "for i in range(0, len(acc_per_fold_train_1)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold_train_1[i]} - Accuracy: {acc_per_fold_train_1[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold_train_1)} (+- {np.std(acc_per_fold_train_1)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold_train_1)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 / CNN1 / GROUP 1...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00307: early stopping\n",
            "Score for fold 1 on TESTING: loss of 0.6495362520217896; accuracy of 66.35547280311584%\n",
            "Score for fold 1 on TRAINING: loss of 0.6238362789154053; accuracy of 66.34399890899658%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 / CNN1 / GROUP 1...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00390: early stopping\n",
            "Score for fold 2 on TESTING: loss of 0.6331994533538818; accuracy of 66.55641198158264%\n",
            "Score for fold 2 on TRAINING: loss of 0.6014804840087891; accuracy of 67.80144572257996%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 / CNN1 / GROUP 1...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00316: early stopping\n",
            "Score for fold 3 on TESTING: loss of 0.6369702816009521; accuracy of 65.80366492271423%\n",
            "Score for fold 3 on TRAINING: loss of 0.6199376583099365; accuracy of 65.75575470924377%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 / CNN1 / GROUP 1...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00443: early stopping\n",
            "Score for fold 4 on TESTING: loss of 0.6188246011734009; accuracy of 66.58371686935425%\n",
            "Score for fold 4 on TRAINING: loss of 0.5642615556716919; accuracy of 71.78542613983154%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 / CNN1 / GROUP 1...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00435: early stopping\n",
            "Score for fold 5 on TESTING: loss of 0.6800692677497864; accuracy of 64.98944163322449%\n",
            "Score for fold 5 on TRAINING: loss of 0.5910364389419556; accuracy of 67.53153204917908%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold on validation set\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.6495362520217896 - Accuracy: 66.35547280311584%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.6331994533538818 - Accuracy: 66.55641198158264%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.6369702816009521 - Accuracy: 65.80366492271423%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.6188246011734009 - Accuracy: 66.58371686935425%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.6800692677497864 - Accuracy: 64.98944163322449%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 66.05774164199829 (+- 0.6032886174464073)\n",
            "> Loss: 0.6437199711799622\n",
            "------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------\n",
            "Score per fold on training set\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.6238362789154053 - Accuracy: 66.34399890899658%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.6014804840087891 - Accuracy: 67.80144572257996%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.6199376583099365 - Accuracy: 65.75575470924377%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.5642615556716919 - Accuracy: 71.78542613983154%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.5910364389419556 - Accuracy: 67.53153204917908%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 67.84363150596619 (+- 2.1092680290092143)\n",
            "> Loss: 0.6001104831695556\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtgzmeNBVrtc",
        "outputId": "5cfa2329-7390-4d82-fce5-83d94c69047b"
      },
      "source": [
        "acc_per_fold_1 = []\n",
        "loss_per_fold_1 = []\n",
        "\n",
        "acc_per_fold_train_1 = []\n",
        "loss_per_fold_train_1 = []\n",
        "\n",
        "# K-fold Cross Validation model evaluation CNN1\n",
        "fold_no = 1\n",
        "for fold_no in range(1, 6):\n",
        "\n",
        "  #LOAD FOLD DATASET\n",
        "  X_test2 = np.load('/content/drive/My Drive/paper/samples/kfold/X_test2_fold' + str(fold_no) + '_7.npy')\n",
        "  y_test2 = np.load('/content/drive/My Drive/paper/samples/kfold/y_test2_fold' + str(fold_no) + '_7.npy')\n",
        "\n",
        "  X_train2 = np.load('/content/drive/My Drive/paper/samples/kfold/X_train2_fold' + str(fold_no) + '_7.npy')\n",
        "  y_train2 = np.load('/content/drive/My Drive/paper/samples/kfold/y_train2_fold' + str(fold_no) + '_7.npy')\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} / CNN1 / GROUP 2...')\n",
        "\n",
        "  CNN1 = cnn1(input_shape = (7, 7, 20), batch_normalization=True, momentum = 0.99, activation = 'relu', loss='binary_crossentropy', lr = 0.0001, optimizer = 'adam')\n",
        "\n",
        "  history_CNN1 = CNN1.fit(X_train2, y_train2, batch_size = 32, epochs = 500, callbacks = [reduce_lr, early_stop], validation_data = (X_test1, y_test1), verbose = 0)\n",
        "  \n",
        "  # Generate generalization metrics on VALIDATION\n",
        "  scores = CNN1.evaluate(X_test2, y_test2, verbose=0)\n",
        "  print(f'Score for fold {fold_no} on TESTING: {CNN1.metrics_names[0]} of {scores[0]}; {CNN1.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold_1.append(scores[1] * 100)\n",
        "  loss_per_fold_1.append(scores[0])\n",
        "\n",
        "  # Generate generalization metrics on TRAINING\n",
        "  scores_train = CNN1.evaluate(X_train2, y_train2, verbose=0)\n",
        "  print(f'Score for fold {fold_no} on TRAINING: {CNN1.metrics_names[0]} of {scores_train[0]}; {CNN1.metrics_names[1]} of {scores_train[1]*100}%')\n",
        "  acc_per_fold_train_1.append(scores_train[1] * 100)\n",
        "  loss_per_fold_train_1.append(scores_train[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold on validation set')\n",
        "for i in range(0, len(acc_per_fold_1)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold_1[i]} - Accuracy: {acc_per_fold_1[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold_1)} (+- {np.std(acc_per_fold_1)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold_1)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold on training set')\n",
        "for i in range(0, len(acc_per_fold_train_1)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold_train_1[i]} - Accuracy: {acc_per_fold_train_1[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold_train_1)} (+- {np.std(acc_per_fold_train_1)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold_train_1)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 / CNN1 / GROUP 2...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00319: early stopping\n",
            "Score for fold 1 on TESTING: loss of 0.6539201736450195; accuracy of 59.15653705596924%\n",
            "Score for fold 1 on TRAINING: loss of 0.6394328474998474; accuracy of 60.742151737213135%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 / CNN1 / GROUP 2...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00253: early stopping\n",
            "Score for fold 2 on TESTING: loss of 0.668300211429596; accuracy of 59.013354778289795%\n",
            "Score for fold 2 on TRAINING: loss of 0.6570395231246948; accuracy of 59.96553897857666%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 / CNN1 / GROUP 2...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00270: early stopping\n",
            "Score for fold 3 on TESTING: loss of 0.6435040831565857; accuracy of 60.73596477508545%\n",
            "Score for fold 3 on TRAINING: loss of 0.6403862833976746; accuracy of 60.83190441131592%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 / CNN1 / GROUP 2...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00392: early stopping\n",
            "Score for fold 4 on TESTING: loss of 0.7844069600105286; accuracy of 62.59600520133972%\n",
            "Score for fold 4 on TRAINING: loss of 0.6025012731552124; accuracy of 65.96128344535828%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 / CNN1 / GROUP 2...\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00308: early stopping\n",
            "Score for fold 5 on TESTING: loss of 1.554870367050171; accuracy of 60.95570921897888%\n",
            "Score for fold 5 on TRAINING: loss of 0.6331567764282227; accuracy of 65.41445851325989%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold on validation set\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.6539201736450195 - Accuracy: 59.15653705596924%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.668300211429596 - Accuracy: 59.013354778289795%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.6435040831565857 - Accuracy: 60.73596477508545%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.7844069600105286 - Accuracy: 62.59600520133972%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 1.554870367050171 - Accuracy: 60.95570921897888%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 60.49151420593262 (+- 1.3169081004458476)\n",
            "> Loss: 0.8610003590583801\n",
            "------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------\n",
            "Score per fold on training set\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.6394328474998474 - Accuracy: 60.742151737213135%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.6570395231246948 - Accuracy: 59.96553897857666%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.6403862833976746 - Accuracy: 60.83190441131592%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.6025012731552124 - Accuracy: 65.96128344535828%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.6331567764282227 - Accuracy: 65.41445851325989%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 62.583067417144775 (+- 2.558754252082568)\n",
            "> Loss: 0.6345033407211303\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBAWnXRp2njj",
        "outputId": "681cd968-64c4-4227-8721-65aef40dc01d"
      },
      "source": [
        "acc_per_fold_1 = []\n",
        "loss_per_fold_1 = []\n",
        "\n",
        "acc_per_fold_train_1 = []\n",
        "loss_per_fold_train_1 = []\n",
        "\n",
        "CNN1 = tf.keras.models.load_model('/content/drive/My Drive/MT/checkpoints/CNN1/CNN1_25')\n",
        "\n",
        "# K-fold Cross Validation model evaluation CNN1\n",
        "fold_no = 1\n",
        "for fold_no in range(1, 6):\n",
        "\n",
        "  #LOAD FOLD DATASET\n",
        "  X_test2 = np.load('/content/drive/My Drive/paper/samples/kfold/X_test2_fold' + str(fold_no) + '.npy')\n",
        "  y_test2 = np.load('/content/drive/My Drive/paper/samples/kfold/y_test2_fold' + str(fold_no) + '.npy')\n",
        "\n",
        "  X_train2 = np.load('/content/drive/My Drive/paper/samples/kfold/X_train2_fold' + str(fold_no) + '.npy')\n",
        "  y_train2 = np.load('/content/drive/My Drive/paper/samples/kfold/y_train2_fold' + str(fold_no) + '.npy')\n",
        "\n",
        "  X_test1 = np.load('/content/drive/My Drive/paper/samples/kfold/X_test1_fold' + str(fold_no) + '.npy')\n",
        "  y_test1 = np.load('/content/drive/My Drive/paper/samples/kfold/y_test1_fold' + str(fold_no) + '.npy')\n",
        "\n",
        "  X_train1 = np.load('/content/drive/My Drive/paper/samples/kfold/X_train1_fold' + str(fold_no) + '.npy')\n",
        "  y_train1 = np.load('/content/drive/My Drive/paper/samples/kfold/y_train1_fold' + str(fold_no) + '.npy')\n",
        "\n",
        "  print('\\n------------------------------------------------------------------------\\n')\n",
        "  print(f'Predictions for fold {fold_no} / with pretrained CNN1 / GROUP 1...')\n",
        "\n",
        "  # Generate generalization metrics on VALIDATION\n",
        "  scores = CNN1.evaluate(X_test1, y_test1, verbose=0)\n",
        "  print(f'Score for fold {fold_no} on TESTING: {CNN1.metrics_names[0]} of {scores[0]}; {CNN1.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold_1.append(scores[1] * 100)\n",
        "  loss_per_fold_1.append(scores[0])\n",
        "\n",
        "  # Generate generalization metrics on TRAINING\n",
        "  scores_train = CNN1.evaluate(X_train1, y_train1, verbose=0)\n",
        "  print(f'Score for fold {fold_no} on TRAINING: {CNN1.metrics_names[0]} of {scores_train[0]}; {CNN1.metrics_names[1]} of {scores_train[1]*100}%')\n",
        "  acc_per_fold_train_1.append(scores_train[1] * 100)\n",
        "  loss_per_fold_train_1.append(scores_train[0])\n",
        "\n",
        "  print('\\n------------------------------------------------------------------------\\n')\n",
        "  print(f'Predictions for fold {fold_no} / with pretrained CNN1 / GROUP 2...')\n",
        "\n",
        "  # Generate generalization metrics on VALIDATION\n",
        "  scores = CNN1.evaluate(X_test2, y_test2, verbose=0)\n",
        "  print(f'Score for fold {fold_no} on TESTING: {CNN1.metrics_names[0]} of {scores[0]}; {CNN1.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold_1.append(scores[1] * 100)\n",
        "  loss_per_fold_1.append(scores[0])\n",
        "\n",
        "  # Generate generalization metrics on TRAINING\n",
        "  scores_train = CNN1.evaluate(X_train2, y_train2, verbose=0)\n",
        "  print(f'Score for fold {fold_no} on TRAINING: {CNN1.metrics_names[0]} of {scores_train[0]}; {CNN1.metrics_names[1]} of {scores_train[1]*100}%')\n",
        "  acc_per_fold_train_1.append(scores_train[1] * 100)\n",
        "  loss_per_fold_train_1.append(scores_train[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold on validation set')\n",
        "for i in range(0, len(acc_per_fold_1)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold_1[i]} - Accuracy: {acc_per_fold_1[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold_1)} (+- {np.std(acc_per_fold_1)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold_1)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold on training set')\n",
        "for i in range(0, len(acc_per_fold_train_1)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold_train_1[i]} - Accuracy: {acc_per_fold_train_1[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold_train_1)} (+- {np.std(acc_per_fold_train_1)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold_train_1)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Predictions for fold 1 / with pretrained CNN1 / GROUP 1...\n",
            "Score for fold 1 on TESTING: loss of 0.17298440635204315; accuracy of 92.9421067237854%\n",
            "Score for fold 1 on TRAINING: loss of 0.17646586894989014; accuracy of 93.12286376953125%\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Predictions for fold 1 / with pretrained CNN1 / GROUP 2...\n",
            "Score for fold 1 on TESTING: loss of 0.2602132558822632; accuracy of 88.48326802253723%\n",
            "Score for fold 1 on TRAINING: loss of 0.24327576160430908; accuracy of 89.82452154159546%\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Predictions for fold 2 / with pretrained CNN1 / GROUP 1...\n",
            "Score for fold 2 on TESTING: loss of 0.1678781509399414; accuracy of 93.62295866012573%\n",
            "Score for fold 2 on TRAINING: loss of 0.17774778604507446; accuracy of 92.951101064682%\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Predictions for fold 2 / with pretrained CNN1 / GROUP 2...\n",
            "Score for fold 2 on TESTING: loss of 0.23094402253627777; accuracy of 90.53586721420288%\n",
            "Score for fold 2 on TRAINING: loss of 0.2505090832710266; accuracy of 89.31747674942017%\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Predictions for fold 3 / with pretrained CNN1 / GROUP 1...\n",
            "Score for fold 3 on TESTING: loss of 0.1717274785041809; accuracy of 93.33871603012085%\n",
            "Score for fold 3 on TRAINING: loss of 0.17676177620887756; accuracy of 93.02372336387634%\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Predictions for fold 3 / with pretrained CNN1 / GROUP 2...\n",
            "Score for fold 3 on TESTING: loss of 0.24161356687545776; accuracy of 90.02557396888733%\n",
            "Score for fold 3 on TRAINING: loss of 0.24788512289524078; accuracy of 89.44199681282043%\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Predictions for fold 4 / with pretrained CNN1 / GROUP 1...\n",
            "Score for fold 4 on TESTING: loss of 0.17884910106658936; accuracy of 93.19999814033508%\n",
            "Score for fold 4 on TRAINING: loss of 0.17498734593391418; accuracy of 93.05778741836548%\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Predictions for fold 4 / with pretrained CNN1 / GROUP 2...\n",
            "Score for fold 4 on TESTING: loss of 0.2517744302749634; accuracy of 88.97976875305176%\n",
            "Score for fold 4 on TRAINING: loss of 0.24533823132514954; accuracy of 89.70398902893066%\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Predictions for fold 5 / with pretrained CNN1 / GROUP 1...\n",
            "Score for fold 5 on TESTING: loss of 0.18757377564907074; accuracy of 92.31707453727722%\n",
            "Score for fold 5 on TRAINING: loss of 0.17285814881324768; accuracy of 93.27538013458252%\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Predictions for fold 5 / with pretrained CNN1 / GROUP 2...\n",
            "Score for fold 5 on TESTING: loss of 0.24864697456359863; accuracy of 89.75878357887268%\n",
            "Score for fold 5 on TRAINING: loss of 0.2461063414812088; accuracy of 89.50883150100708%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold on validation set\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.17298440635204315 - Accuracy: 92.9421067237854%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.2602132558822632 - Accuracy: 88.48326802253723%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.1678781509399414 - Accuracy: 93.62295866012573%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.23094402253627777 - Accuracy: 90.53586721420288%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.1717274785041809 - Accuracy: 93.33871603012085%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.24161356687545776 - Accuracy: 90.02557396888733%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.17884910106658936 - Accuracy: 93.19999814033508%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.2517744302749634 - Accuracy: 88.97976875305176%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.18757377564907074 - Accuracy: 92.31707453727722%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.24864697456359863 - Accuracy: 89.75878357887268%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 91.32041156291962 (+- 1.8652115821297268)\n",
            "> Loss: 0.21122051626443863\n",
            "------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------\n",
            "Score per fold on training set\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.17646586894989014 - Accuracy: 93.12286376953125%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.24327576160430908 - Accuracy: 89.82452154159546%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.17774778604507446 - Accuracy: 92.951101064682%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.2505090832710266 - Accuracy: 89.31747674942017%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.17676177620887756 - Accuracy: 93.02372336387634%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.24788512289524078 - Accuracy: 89.44199681282043%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.17498734593391418 - Accuracy: 93.05778741836548%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.24533823132514954 - Accuracy: 89.70398902893066%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.17285814881324768 - Accuracy: 93.27538013458252%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.2461063414812088 - Accuracy: 89.50883150100708%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 91.32276713848114 (+- 1.769804953010107)\n",
            "> Loss: 0.21119354665279388\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPPdoYSE5mWs",
        "outputId": "65460605-5d0e-4686-b2d1-3e1d7be7c7fd"
      },
      "source": [
        "acc_per_fold_1 = []\n",
        "loss_per_fold_1 = []\n",
        "\n",
        "acc_per_fold_train_1 = []\n",
        "loss_per_fold_train_1 = []\n",
        "\n",
        "CNN1 = tf.keras.models.load_model('/content/drive/My Drive/MT/checkpoints/CNN1/CNN1_25')\n",
        "\n",
        "# K-fold Cross Validation model evaluation CNN1\n",
        "fold_no = 1\n",
        "for fold_no in range(1, 6):\n",
        "\n",
        "  #LOAD FOLD DATASET\n",
        "  X_test2 = np.load('/content/drive/My Drive/paper/samples/kfold/X_test2_fold' + str(fold_no) + '.npy')\n",
        "  y_test2 = np.load('/content/drive/My Drive/paper/samples/kfold/y_test2_fold' + str(fold_no) + '.npy')\n",
        "\n",
        "  X_train2 = np.load('/content/drive/My Drive/paper/samples/kfold/X_train2_fold' + str(fold_no) + '.npy')\n",
        "  y_train2 = np.load('/content/drive/My Drive/paper/samples/kfold/y_train2_fold' + str(fold_no) + '.npy')\n",
        "\n",
        "  X_test1 = np.load('/content/drive/My Drive/paper/samples/kfold/X_test1_fold' + str(fold_no) + '.npy')\n",
        "  y_test1 = np.load('/content/drive/My Drive/paper/samples/kfold/y_test1_fold' + str(fold_no) + '.npy')\n",
        "\n",
        "  X_train1 = np.load('/content/drive/My Drive/paper/samples/kfold/X_train1_fold' + str(fold_no) + '.npy')\n",
        "  y_train1 = np.load('/content/drive/My Drive/paper/samples/kfold/y_train1_fold' + str(fold_no) + '.npy')\n",
        "\n",
        "  X_train = np.concatenate((X_train1, X_train2), axis = 0)\n",
        "  X_test = np.concatenate((X_test1, X_test2), axis = 0)\n",
        "  y_train = np.concatenate((y_train1, y_train2), axis = 0)\n",
        "  y_test = np.concatenate((y_test1, y_test2), axis = 0)\n",
        "\n",
        "  print('\\n------------------------------------------------------------------------\\n')\n",
        "  print('Training set size', X_train.shape[0])\n",
        "  print('Testing set size', X_test.shape[0])\n",
        "  print('\\n------------------------------------------------------------------------\\n')\n",
        "\n",
        "  print('\\n------------------------------------------------------------------------\\n')\n",
        "  print(f'Predictions for fold {fold_no} / with pretrained CNN1 / Complete dataset...')\n",
        "\n",
        "  # Generate generalization metrics on VALIDATION\n",
        "  scores = CNN1.evaluate(X_test, y_test, verbose=0)\n",
        "  print(f'Score for fold {fold_no} on TESTING: {CNN1.metrics_names[0]} of {scores[0]}; {CNN1.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold_1.append(scores[1] * 100)\n",
        "  loss_per_fold_1.append(scores[0])\n",
        "\n",
        "  # Generate generalization metrics on TRAINING\n",
        "  scores_train = CNN1.evaluate(X_train, y_train, verbose=0)\n",
        "  print(f'Score for fold {fold_no} on TRAINING: {CNN1.metrics_names[0]} of {scores_train[0]}; {CNN1.metrics_names[1]} of {scores_train[1]*100}%')\n",
        "  acc_per_fold_train_1.append(scores_train[1] * 100)\n",
        "  loss_per_fold_train_1.append(scores_train[0])\n",
        "\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold on validation set')\n",
        "for i in range(0, len(acc_per_fold_1)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold_1[i]} - Accuracy: {acc_per_fold_1[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold_1)} (+- {np.std(acc_per_fold_1)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold_1)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold on training set')\n",
        "for i in range(0, len(acc_per_fold_train_1)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold_train_1[i]} - Accuracy: {acc_per_fold_train_1[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold_train_1)} (+- {np.std(acc_per_fold_train_1)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold_train_1)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Training set size 19292\n",
            "Testing set size 4823\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Predictions for fold 1 / with pretrained CNN1 / Complete dataset...\n",
            "Score for fold 1 on TESTING: loss of 0.2146003395318985; accuracy of 90.8148467540741%\n",
            "Score for fold 1 on TRAINING: loss of 0.2088318169116974; accuracy of 91.52498245239258%\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Training set size 19292\n",
            "Testing set size 4823\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Predictions for fold 2 / with pretrained CNN1 / Complete dataset...\n",
            "Score for fold 2 on TESTING: loss of 0.19813622534275055; accuracy of 92.14181900024414%\n",
            "Score for fold 2 on TRAINING: loss of 0.2129478007555008; accuracy of 91.19324088096619%\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Training set size 19292\n",
            "Testing set size 4823\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Predictions for fold 3 / with pretrained CNN1 / Complete dataset...\n",
            "Score for fold 3 on TESTING: loss of 0.20572136342525482; accuracy of 91.72714352607727%\n",
            "Score for fold 3 on TRAINING: loss of 0.21105150878429413; accuracy of 91.29691123962402%\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Training set size 19292\n",
            "Testing set size 4823\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Predictions for fold 4 / with pretrained CNN1 / Complete dataset...\n",
            "Score for fold 4 on TESTING: loss of 0.2139735370874405; accuracy of 91.16732478141785%\n",
            "Score for fold 4 on TRAINING: loss of 0.2089884728193283; accuracy of 91.43686294555664%\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Training set size 19292\n",
            "Testing set size 4823\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Predictions for fold 5 / with pretrained CNN1 / Complete dataset...\n",
            "Score for fold 5 on TESTING: loss of 0.21749621629714966; accuracy of 91.06365442276001%\n",
            "Score for fold 5 on TRAINING: loss of 0.20810791850090027; accuracy of 91.46278500556946%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold on validation set\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.2146003395318985 - Accuracy: 90.8148467540741%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.19813622534275055 - Accuracy: 92.14181900024414%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.20572136342525482 - Accuracy: 91.72714352607727%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.2139735370874405 - Accuracy: 91.16732478141785%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.21749621629714966 - Accuracy: 91.06365442276001%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 91.38295769691467 (+- 0.4828118766546658)\n",
            "> Loss: 0.20998553633689881\n",
            "------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------\n",
            "Score per fold on training set\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.2088318169116974 - Accuracy: 91.52498245239258%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.2129478007555008 - Accuracy: 91.19324088096619%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.21105150878429413 - Accuracy: 91.29691123962402%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.2089884728193283 - Accuracy: 91.43686294555664%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.20810791850090027 - Accuracy: 91.46278500556946%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 91.38295650482178 (+- 0.12070266713339474)\n",
            "> Loss: 0.20998550355434417\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X6LyQHD_bHB"
      },
      "source": [
        "modelcnn.load_weights('/content/drive/My Drive/MT/checkpoints/CNN1/CNNZhang25x25-8220')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "F7TizU-e_rUD",
        "outputId": "c210b107-0a64-40ff-de49-0aec32fcdaf4"
      },
      "source": [
        "modelcnn.save('/content/drive/My Drive/MT/checkpoints/CNN1/CNN1_25')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/MT/checkpoints/CNN1/CNN1_25/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ZD1t0-jkY3-d",
        "outputId": "59953db8-ab42-434b-b1a4-06e7d67cd405"
      },
      "source": [
        "january = np.load('/content/drive/My Drive/MT/2019/Jan2019_v2.npy')\n",
        "january.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1187, 710, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Ksi2pZm2EX6P",
        "outputId": "78eede98-01a8-4879-8698-f9ba61e003d5"
      },
      "source": [
        "model = build_model1(input_shape = (25, 25, 20), batch_normalization=True, momentum = 0.99, activation = 'relu', loss='binary_crossentropy', lr = 0.001, optimizer= 'adam')\n",
        "\n",
        "model.load_weights('/content/drive/My Drive/MT/checkpoints/CNNZhang25x25-8220')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fdb603c80f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB6tlrTmJi5J"
      },
      "source": [
        "map = np.zeros((1163, 686))\n",
        "i = 0\n",
        "j = 0\n",
        "patch_size = 25\n",
        "margin = int(patch_size/2 - 0.5)\n",
        "\n",
        "for i in range(january.shape[0] - patch_size + 1):\n",
        "  for j in range(january.shape[1] - patch_size + 1):\n",
        "    patch = january[i:i + patch_size, j:j + patch_size, :]\n",
        "    patch = np.expand_dims(patch, axis=0)\n",
        "    probability = model(patch, training = False)\n",
        "    map[i,j] = probability\n",
        "\n",
        "map1 = np.pad(map, 12, mode='constant')\n",
        "print(map1.shape)\n",
        "\n",
        "np.save('/content/drive/My Drive/MT/CNN25-Jan2019.npy', map1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85TdGW1oEeQ6"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm_matrix = confusion_matrix(y_test, predict_class)\n",
        "df_cm = pd.DataFrame(cm_matrix, ['0', '1'], ['0', '1'])\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True, cmap=\"YlGnBu\", fmt = 'd')\n",
        "\n",
        "tn, fp, fn, tp = cm_matrix.ravel()\n",
        "print('True positives:', tn, \"False positives:\", fp, 'False negatives:', fn, 'True negatives:', tn)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}